{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Need to remove stop words and punctuation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import collections\n",
    "import math\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "vocabulary_size = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_symbols(doc):\n",
    "    cleaned_doc = []\n",
    "    punctuation = ['.', '..', '...', '?', ':', '!']\n",
    "    for word in doc:\n",
    "        if word not in punctuation:\n",
    "            cleaned_doc.append(re.sub(r'\\.', '', word))\n",
    "    return cleaned_doc\n",
    "def clean_doc(doc):\n",
    "    words = re.sub(r'\\.(?=[^ \\W\\d])', '. ', doc).lower()\n",
    "    words = word_tokenize(re.sub(r'\\.(?=[^ \\W\\d])', '. ', doc))\n",
    "    words = remove_symbols(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['at',\n",
       " 'the',\n",
       " 'turn',\n",
       " 'of',\n",
       " 'the',\n",
       " '20th',\n",
       " 'century',\n",
       " 'a',\n",
       " 'man',\n",
       " 'named',\n",
       " 'alfred',\n",
       " 'was',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'preeminent',\n",
       " 'scientists',\n",
       " 'and',\n",
       " 'entrepreneurs',\n",
       " 'of',\n",
       " 'his',\n",
       " 'day',\n",
       " 'he',\n",
       " 'made',\n",
       " 'his',\n",
       " 'fortune',\n",
       " 'by',\n",
       " 'inventing',\n",
       " 'and',\n",
       " 'refining',\n",
       " 'explosiveshis',\n",
       " 'most',\n",
       " 'famous',\n",
       " 'invention',\n",
       " 'being',\n",
       " 'dynamite',\n",
       " 'his',\n",
       " 'intentions',\n",
       " 'were',\n",
       " 'that',\n",
       " 'these',\n",
       " 'explosives',\n",
       " 'were',\n",
       " 'to',\n",
       " 'be',\n",
       " 'used',\n",
       " 'for',\n",
       " 'constructive',\n",
       " 'purposes',\n",
       " 'like',\n",
       " 'building',\n",
       " 'highways',\n",
       " 'and',\n",
       " 'laying',\n",
       " 'foundations',\n",
       " 'for',\n",
       " 'buildings',\n",
       " 'but',\n",
       " 'soon',\n",
       " 'their',\n",
       " 'value',\n",
       " 'for',\n",
       " 'warfare',\n",
       " 'became',\n",
       " 'evident',\n",
       " 'and',\n",
       " 'most',\n",
       " 'of',\n",
       " 'alfreds',\n",
       " 'money',\n",
       " 'was',\n",
       " 'made',\n",
       " 'by',\n",
       " 'selling',\n",
       " 'his',\n",
       " 'material',\n",
       " 'and',\n",
       " 'devices',\n",
       " 'to',\n",
       " 'the',\n",
       " 'military',\n",
       " 'toward',\n",
       " 'the',\n",
       " 'end',\n",
       " 'of',\n",
       " 'his',\n",
       " 'life',\n",
       " 'alfred',\n",
       " 'began',\n",
       " 'to',\n",
       " 'ponder',\n",
       " 'his',\n",
       " 'legacy',\n",
       " 'to',\n",
       " 'humankind',\n",
       " 'while',\n",
       " 'his',\n",
       " 'inventions',\n",
       " 'were',\n",
       " 'meant',\n",
       " 'for',\n",
       " 'good',\n",
       " 'and',\n",
       " 'had',\n",
       " 'been',\n",
       " 'used',\n",
       " 'for',\n",
       " 'good',\n",
       " 'it',\n",
       " 'was',\n",
       " 'also',\n",
       " 'true',\n",
       " 'that',\n",
       " 'his',\n",
       " 'inventions',\n",
       " 'had',\n",
       " 'equipped',\n",
       " 'armies',\n",
       " 'of',\n",
       " 'the',\n",
       " 'world',\n",
       " 'to',\n",
       " 'deliver',\n",
       " 'new',\n",
       " 'and',\n",
       " 'improved',\n",
       " 'forms',\n",
       " 'of',\n",
       " 'death',\n",
       " 'and',\n",
       " 'destruction',\n",
       " 'was',\n",
       " 'that',\n",
       " 'how',\n",
       " 'he',\n",
       " 'wanted',\n",
       " 'to',\n",
       " 'be',\n",
       " 'remembered',\n",
       " 'so',\n",
       " 'alfred',\n",
       " 'rewrote',\n",
       " 'his',\n",
       " 'will',\n",
       " 'and',\n",
       " 'used',\n",
       " 'the',\n",
       " 'bulk',\n",
       " 'of',\n",
       " 'his',\n",
       " 'fortune',\n",
       " 'to',\n",
       " 'establish',\n",
       " 'a',\n",
       " 'series',\n",
       " 'of',\n",
       " 'international',\n",
       " 'awards',\n",
       " 'to',\n",
       " 'be',\n",
       " 'given',\n",
       " 'each',\n",
       " 'year',\n",
       " 'to',\n",
       " 'scientists',\n",
       " 'thinkers',\n",
       " 'and',\n",
       " 'leaders',\n",
       " 'who',\n",
       " 'had',\n",
       " 'made',\n",
       " 'a',\n",
       " 'remarkable',\n",
       " 'contribution',\n",
       " 'to',\n",
       " 'the',\n",
       " 'betterment',\n",
       " 'of',\n",
       " 'humankind',\n",
       " 'that',\n",
       " 'mans',\n",
       " 'name',\n",
       " 'was',\n",
       " 'alfred',\n",
       " 'nobel',\n",
       " 'and',\n",
       " 'the',\n",
       " 'most',\n",
       " 'famous',\n",
       " 'of',\n",
       " 'those',\n",
       " 'awards',\n",
       " 'is',\n",
       " 'given',\n",
       " 'to',\n",
       " 'the',\n",
       " 'person',\n",
       " 'or',\n",
       " 'persons',\n",
       " 'who',\n",
       " 'shall',\n",
       " 'have',\n",
       " 'done',\n",
       " 'the',\n",
       " 'most',\n",
       " 'or',\n",
       " 'the',\n",
       " 'best',\n",
       " 'work',\n",
       " 'for',\n",
       " 'fraternity',\n",
       " 'between',\n",
       " 'the',\n",
       " 'nations',\n",
       " 'for',\n",
       " 'the',\n",
       " 'abolition',\n",
       " 'or',\n",
       " 'reduction',\n",
       " 'of',\n",
       " 'standing',\n",
       " 'armies',\n",
       " 'and',\n",
       " 'for',\n",
       " 'the',\n",
       " 'promotion',\n",
       " 'of',\n",
       " 'peace',\n",
       " 'we',\n",
       " 'call',\n",
       " 'it',\n",
       " 'the',\n",
       " 'nobel',\n",
       " 'peace',\n",
       " 'prize',\n",
       " 'and',\n",
       " 'it',\n",
       " 'is',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'highest',\n",
       " 'honors',\n",
       " 'bestowed',\n",
       " 'on',\n",
       " 'a',\n",
       " 'human',\n",
       " 'being',\n",
       " 'the',\n",
       " 'first',\n",
       " 'nobel',\n",
       " 'peace',\n",
       " 'prize',\n",
       " 'went',\n",
       " 'to',\n",
       " 'jean',\n",
       " 'henry',\n",
       " 'dunant',\n",
       " 'a',\n",
       " 'swiss',\n",
       " 'who',\n",
       " 'helped',\n",
       " 'to',\n",
       " 'found',\n",
       " 'the',\n",
       " 'international',\n",
       " 'red',\n",
       " 'cross',\n",
       " 'and',\n",
       " 'the',\n",
       " 'geneva',\n",
       " 'convention',\n",
       " 'in',\n",
       " '1901',\n",
       " 'in',\n",
       " 'recent',\n",
       " 'years',\n",
       " 'the',\n",
       " 'peace',\n",
       " 'prize',\n",
       " 'has',\n",
       " 'been',\n",
       " 'given',\n",
       " 'to',\n",
       " 'people',\n",
       " 'like',\n",
       " 'mother',\n",
       " 'teresa',\n",
       " 'for',\n",
       " 'her',\n",
       " 'work',\n",
       " 'on',\n",
       " 'behalf',\n",
       " 'of',\n",
       " 'the',\n",
       " 'worlds',\n",
       " 'poor',\n",
       " 'elie',\n",
       " 'wiesel',\n",
       " 'the',\n",
       " 'holocaust',\n",
       " 'survivor',\n",
       " 'and',\n",
       " 'human',\n",
       " 'rights',\n",
       " 'advocate',\n",
       " 'nelson',\n",
       " 'mandela',\n",
       " 'for',\n",
       " 'his',\n",
       " 'work',\n",
       " 'in',\n",
       " 'dismantling',\n",
       " 'apartheid',\n",
       " 'in',\n",
       " 'south',\n",
       " 'africa',\n",
       " 'the',\n",
       " 'unlikely',\n",
       " 'trio',\n",
       " 'of',\n",
       " 'yassir',\n",
       " 'arafat',\n",
       " 'shimon',\n",
       " 'peres',\n",
       " 'and',\n",
       " 'yitzhak',\n",
       " 'rabin',\n",
       " 'for',\n",
       " 'their',\n",
       " 'efforts',\n",
       " 'to',\n",
       " 'bring',\n",
       " 'peace',\n",
       " 'to',\n",
       " 'the',\n",
       " 'middle',\n",
       " 'east',\n",
       " 'pres',\n",
       " 'kim',\n",
       " 'daejung',\n",
       " 'of',\n",
       " 'south',\n",
       " 'korea',\n",
       " 'for',\n",
       " 'his',\n",
       " 'human',\n",
       " 'rights',\n",
       " 'work',\n",
       " 'throughout',\n",
       " 'east',\n",
       " 'asia',\n",
       " 'and',\n",
       " 'most',\n",
       " 'recently',\n",
       " 'al',\n",
       " 'gore',\n",
       " 'jr',\n",
       " 'for',\n",
       " 'his',\n",
       " 'research',\n",
       " 'and',\n",
       " 'advocacy',\n",
       " 'related',\n",
       " 'to',\n",
       " 'global',\n",
       " 'warming',\n",
       " 'and',\n",
       " 'its',\n",
       " 'impact',\n",
       " 'on',\n",
       " 'humankind',\n",
       " 'especially',\n",
       " 'the',\n",
       " 'poor',\n",
       " 'what',\n",
       " 'a',\n",
       " 'list',\n",
       " 'each',\n",
       " 'of',\n",
       " 'these',\n",
       " 'individuals',\n",
       " 'has',\n",
       " 'used',\n",
       " 'their',\n",
       " 'minds',\n",
       " 'talents',\n",
       " 'and',\n",
       " 'influence',\n",
       " 'to',\n",
       " 'advance',\n",
       " 'the',\n",
       " 'cause',\n",
       " 'of',\n",
       " 'world',\n",
       " 'peace',\n",
       " 'and',\n",
       " 'the',\n",
       " 'betterment',\n",
       " 'of',\n",
       " 'humankind',\n",
       " 'but',\n",
       " 'after',\n",
       " 'reading',\n",
       " 'through',\n",
       " 'that',\n",
       " 'list',\n",
       " 'one',\n",
       " 'unsettling',\n",
       " 'fact',\n",
       " 'remains',\n",
       " 'after',\n",
       " 'all',\n",
       " 'that',\n",
       " 'these',\n",
       " 'individuals',\n",
       " 'have',\n",
       " 'accomplished',\n",
       " 'we',\n",
       " 'are',\n",
       " 'no',\n",
       " 'closer',\n",
       " 'to',\n",
       " 'world',\n",
       " 'peace',\n",
       " 'today',\n",
       " 'than',\n",
       " 'we',\n",
       " 'were',\n",
       " '100',\n",
       " 'years',\n",
       " 'ago',\n",
       " 'the',\n",
       " 'world',\n",
       " 'is',\n",
       " 'as',\n",
       " 'violent',\n",
       " 'volatile',\n",
       " 'and',\n",
       " 'frightening',\n",
       " 'as',\n",
       " 'ever',\n",
       " 'if',\n",
       " 'not',\n",
       " 'more',\n",
       " 'so',\n",
       " 'peace',\n",
       " 'continues',\n",
       " 'to',\n",
       " 'be',\n",
       " 'the',\n",
       " 'most',\n",
       " 'elusive',\n",
       " 'of',\n",
       " 'all',\n",
       " 'human',\n",
       " 'ambitions',\n",
       " 'what',\n",
       " 'exactly',\n",
       " 'did',\n",
       " 'jesus',\n",
       " 'mean',\n",
       " 'when',\n",
       " 'he',\n",
       " 'said',\n",
       " 'blessed',\n",
       " 'are',\n",
       " 'the',\n",
       " 'peacemakers',\n",
       " 'for',\n",
       " 'they',\n",
       " 'will',\n",
       " 'be',\n",
       " 'called',\n",
       " 'sons',\n",
       " 'of',\n",
       " 'god',\n",
       " 'he',\n",
       " 'makes',\n",
       " 'it',\n",
       " 'sound',\n",
       " 'as',\n",
       " 'if',\n",
       " 'its',\n",
       " 'really',\n",
       " 'possiblethat',\n",
       " 'human',\n",
       " 'beings',\n",
       " 'can',\n",
       " 'bring',\n",
       " 'about',\n",
       " 'peace',\n",
       " 'between',\n",
       " 'people',\n",
       " 'and',\n",
       " 'nations',\n",
       " 'but',\n",
       " 'we',\n",
       " 'just',\n",
       " 'rattled',\n",
       " 'off',\n",
       " 'the',\n",
       " 'names',\n",
       " 'of',\n",
       " 'some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'most',\n",
       " 'brilliant',\n",
       " 'powerful',\n",
       " 'and',\n",
       " 'dedicated',\n",
       " 'men',\n",
       " 'and',\n",
       " 'women',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " 'and',\n",
       " 'their',\n",
       " 'best',\n",
       " 'efforts',\n",
       " 'over',\n",
       " 'the',\n",
       " 'course',\n",
       " 'of',\n",
       " '100',\n",
       " 'years',\n",
       " 'have',\n",
       " 'brought',\n",
       " 'us',\n",
       " 'no',\n",
       " 'closer',\n",
       " 'to',\n",
       " 'world',\n",
       " 'peace',\n",
       " 'if',\n",
       " 'they',\n",
       " 'couldnt',\n",
       " 'do',\n",
       " 'it',\n",
       " 'with',\n",
       " 'all',\n",
       " 'their',\n",
       " 'brainpower',\n",
       " 'and',\n",
       " 'influence',\n",
       " 'and',\n",
       " 'dedication',\n",
       " 'how',\n",
       " 'can',\n",
       " 'ordinary',\n",
       " 'people',\n",
       " 'like',\n",
       " 'you',\n",
       " 'and',\n",
       " 'me',\n",
       " 'make',\n",
       " 'peace',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " 'what',\n",
       " 'were',\n",
       " 'going',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'is',\n",
       " 'that',\n",
       " 'peacemaking',\n",
       " 'begins',\n",
       " 'with',\n",
       " 'a',\n",
       " 'change',\n",
       " 'of',\n",
       " 'heart',\n",
       " 'first',\n",
       " 'notice',\n",
       " 'what',\n",
       " 'jesus',\n",
       " 'didnt',\n",
       " 'say',\n",
       " 'he',\n",
       " 'didnt',\n",
       " 'say',\n",
       " 'blessed',\n",
       " 'are',\n",
       " 'the',\n",
       " 'peaceful',\n",
       " 'for',\n",
       " 'they',\n",
       " 'shall',\n",
       " 'get',\n",
       " 'a',\n",
       " 'good',\n",
       " 'nights',\n",
       " 'sleep',\n",
       " 'peace',\n",
       " 'of',\n",
       " 'mind',\n",
       " 'is',\n",
       " 'a',\n",
       " 'wonderful',\n",
       " 'thing',\n",
       " 'and',\n",
       " 'peacemaking',\n",
       " 'often',\n",
       " 'begins',\n",
       " 'with',\n",
       " 'inner',\n",
       " 'peace',\n",
       " 'but',\n",
       " 'jesus',\n",
       " 'wasnt',\n",
       " 'pronouncing',\n",
       " 'a',\n",
       " 'blessing',\n",
       " 'on',\n",
       " 'peaceful',\n",
       " 'people',\n",
       " 'secondly',\n",
       " 'jesus',\n",
       " 'didnt',\n",
       " 'say',\n",
       " 'blessed',\n",
       " 'are',\n",
       " 'the',\n",
       " 'peaceable',\n",
       " 'for',\n",
       " 'nothing',\n",
       " 'seems',\n",
       " 'to',\n",
       " 'bother',\n",
       " 'them',\n",
       " 'hes',\n",
       " 'not',\n",
       " 'calling',\n",
       " 'for',\n",
       " 'an',\n",
       " 'easygoing',\n",
       " 'no',\n",
       " 'worries',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'temperament',\n",
       " 'everybody',\n",
       " 'likes',\n",
       " 'peaceable',\n",
       " 'people',\n",
       " 'but',\n",
       " 'they',\n",
       " 'dont',\n",
       " 'often',\n",
       " 'change',\n",
       " 'the',\n",
       " 'world',\n",
       " 'thirdly',\n",
       " 'jesus',\n",
       " 'didnt',\n",
       " 'say',\n",
       " 'blessed',\n",
       " 'are',\n",
       " 'the',\n",
       " 'peaceloving',\n",
       " 'for',\n",
       " 'they',\n",
       " 'shall',\n",
       " 'stay',\n",
       " 'out',\n",
       " 'of',\n",
       " 'trouble',\n",
       " 'we',\n",
       " 'all',\n",
       " 'love',\n",
       " 'peacewe',\n",
       " 'prefer',\n",
       " 'it',\n",
       " 'when',\n",
       " 'people',\n",
       " 'and',\n",
       " 'nations',\n",
       " 'get',\n",
       " 'alongbut',\n",
       " 'theres',\n",
       " 'more',\n",
       " 'to',\n",
       " 'making',\n",
       " 'peace',\n",
       " 'than',\n",
       " 'avoiding',\n",
       " 'conflict',\n",
       " 'jesus',\n",
       " 'didnt',\n",
       " 'say',\n",
       " 'blessed',\n",
       " 'are',\n",
       " 'the',\n",
       " 'peaceful',\n",
       " 'or',\n",
       " 'the',\n",
       " 'peaceable',\n",
       " 'or',\n",
       " 'the',\n",
       " 'peaceloving',\n",
       " 'he',\n",
       " 'said',\n",
       " 'blessed',\n",
       " 'are',\n",
       " 'the',\n",
       " 'peacemakers',\n",
       " 'its',\n",
       " 'an',\n",
       " 'active',\n",
       " 'word',\n",
       " 'it',\n",
       " 'implies',\n",
       " 'initiative',\n",
       " 'intervention',\n",
       " 'and',\n",
       " 'risk',\n",
       " 'peacemaking',\n",
       " 'is',\n",
       " 'not',\n",
       " 'for',\n",
       " 'cowards',\n",
       " 'peacemakers',\n",
       " 'are',\n",
       " 'often',\n",
       " 'misunderstood',\n",
       " 'and',\n",
       " 'unappreciated',\n",
       " 'sometimes',\n",
       " 'they',\n",
       " 'get',\n",
       " 'caught',\n",
       " 'in',\n",
       " 'the',\n",
       " 'crossfire',\n",
       " 'we',\n",
       " 'tend',\n",
       " 'to',\n",
       " 'think',\n",
       " 'of',\n",
       " 'peace',\n",
       " 'as',\n",
       " 'the',\n",
       " 'opposite',\n",
       " 'of',\n",
       " 'war',\n",
       " 'or',\n",
       " 'the',\n",
       " 'absence',\n",
       " 'of',\n",
       " 'conflict',\n",
       " 'a',\n",
       " 'cynic',\n",
       " 'once',\n",
       " 'observed',\n",
       " 'that',\n",
       " 'peace',\n",
       " 'is',\n",
       " 'the',\n",
       " 'pause',\n",
       " 'that',\n",
       " 'gives',\n",
       " 'nations',\n",
       " 'time',\n",
       " 'to',\n",
       " 'reload',\n",
       " 'but',\n",
       " 'when',\n",
       " 'jesus',\n",
       " 'uses',\n",
       " 'the',\n",
       " 'word',\n",
       " 'peace',\n",
       " 'here',\n",
       " 'he',\n",
       " 'has',\n",
       " 'in',\n",
       " 'mind',\n",
       " 'the',\n",
       " 'old',\n",
       " 'testament',\n",
       " 'idea',\n",
       " 'of',\n",
       " 'peace',\n",
       " 'captured',\n",
       " 'in',\n",
       " 'the',\n",
       " 'hebrew',\n",
       " 'word',\n",
       " 'shalom',\n",
       " 'shalom',\n",
       " 'is',\n",
       " 'one',\n",
       " 'of',\n",
       " 'gods',\n",
       " 'favorite',\n",
       " 'words',\n",
       " 'appearing',\n",
       " '250',\n",
       " 'times',\n",
       " 'in',\n",
       " 'the',\n",
       " 'old',\n",
       " 'testament',\n",
       " 'its',\n",
       " 'the',\n",
       " 'promise',\n",
       " 'of',\n",
       " 'that',\n",
       " 'great',\n",
       " 'benediction',\n",
       " 'may',\n",
       " 'the',\n",
       " 'lord',\n",
       " 'bless',\n",
       " 'you',\n",
       " 'and',\n",
       " 'keep',\n",
       " 'you',\n",
       " 'may',\n",
       " 'the',\n",
       " 'lord',\n",
       " 'make',\n",
       " 'his',\n",
       " 'face',\n",
       " 'shine',\n",
       " 'upon',\n",
       " 'you',\n",
       " 'and',\n",
       " 'be',\n",
       " 'gracious',\n",
       " 'to',\n",
       " 'you',\n",
       " 'may',\n",
       " 'the',\n",
       " 'lord',\n",
       " 'turn',\n",
       " 'his',\n",
       " 'face',\n",
       " 'toward',\n",
       " 'you',\n",
       " 'and',\n",
       " 'give',\n",
       " 'you',\n",
       " 'his',\n",
       " 'peace',\n",
       " 'shalom',\n",
       " 'is',\n",
       " 'much',\n",
       " 'more',\n",
       " 'than',\n",
       " 'the',\n",
       " 'absence',\n",
       " 'of',\n",
       " 'conflict',\n",
       " 'shalom',\n",
       " 'means',\n",
       " 'wholeness',\n",
       " 'and',\n",
       " 'wellness',\n",
       " 'to',\n",
       " 'experience',\n",
       " 'shalom',\n",
       " 'is',\n",
       " 'to',\n",
       " 'be',\n",
       " 'in',\n",
       " 'harmony',\n",
       " 'with',\n",
       " 'god',\n",
       " 'with',\n",
       " 'others',\n",
       " 'and',\n",
       " 'with',\n",
       " 'yourself',\n",
       " 'shalom',\n",
       " 'is',\n",
       " 'global',\n",
       " 'and',\n",
       " 'its',\n",
       " 'personal',\n",
       " 'its',\n",
       " 'verticalbetween',\n",
       " 'people',\n",
       " 'and',\n",
       " 'godand',\n",
       " 'its',\n",
       " 'horizontalencompassing',\n",
       " 'relations',\n",
       " 'among',\n",
       " 'people',\n",
       " 'shalom',\n",
       " 'means',\n",
       " 'everything',\n",
       " 'is',\n",
       " 'in',\n",
       " 'order',\n",
       " 'everything',\n",
       " 'is',\n",
       " 'as',\n",
       " 'it',\n",
       " 'should',\n",
       " 'be',\n",
       " 'shalom',\n",
       " 'is',\n",
       " 'essentially',\n",
       " 'the',\n",
       " 'old',\n",
       " 'testament',\n",
       " 'word',\n",
       " 'for',\n",
       " 'salvation',\n",
       " 'it',\n",
       " 'is',\n",
       " 'to',\n",
       " 'be',\n",
       " 'rightly',\n",
       " 'related',\n",
       " 'to',\n",
       " 'god',\n",
       " 'and',\n",
       " 'others',\n",
       " 'so',\n",
       " 'that',\n",
       " 'gods',\n",
       " 'gifts',\n",
       " 'flow',\n",
       " 'freely',\n",
       " 'to',\n",
       " 'you',\n",
       " 'and',\n",
       " 'through',\n",
       " 'you',\n",
       " 'peacemakers',\n",
       " 'then',\n",
       " 'are',\n",
       " 'those',\n",
       " 'who',\n",
       " 'actively',\n",
       " 'and',\n",
       " 'effectively',\n",
       " 'extend',\n",
       " 'gods',\n",
       " 'shalom',\n",
       " 'in',\n",
       " 'the',\n",
       " 'worldwho',\n",
       " 'spread',\n",
       " 'the',\n",
       " 'tent',\n",
       " 'of',\n",
       " 'gods',\n",
       " 'blessing',\n",
       " 'wider',\n",
       " 'and',\n",
       " 'wider',\n",
       " 'so',\n",
       " 'that',\n",
       " 'more',\n",
       " 'and',\n",
       " 'more',\n",
       " 'people',\n",
       " 'come',\n",
       " 'under',\n",
       " 'its',\n",
       " 'shade',\n",
       " 'where',\n",
       " 'they',\n",
       " 'find',\n",
       " 'peace',\n",
       " 'with',\n",
       " 'god',\n",
       " 'and',\n",
       " 'with',\n",
       " 'each',\n",
       " 'other',\n",
       " 'one',\n",
       " 'commentator',\n",
       " 'says',\n",
       " 'that',\n",
       " 'peacemaking',\n",
       " 'means',\n",
       " 'to',\n",
       " 'be',\n",
       " 'actively',\n",
       " 'engaged',\n",
       " 'in',\n",
       " 'bringing',\n",
       " 'all',\n",
       " 'gods',\n",
       " 'redemptive',\n",
       " 'purposes',\n",
       " 'to',\n",
       " 'bear',\n",
       " 'on',\n",
       " 'human',\n",
       " 'society',\n",
       " ...]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "f=open('../sermons/preachingtoday/A_Better_Peace_Prize.json', 'rb')\n",
    "contents =f.read()\n",
    "contents = json.loads(contents)\n",
    "tokens = clean_doc(contents['text'])\n",
    "for word in range(len(tokens)):\n",
    "    if tokens[word] != '.':\n",
    "        data.append(tokens[word].lower())\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    data = []\n",
    "    for sermon in os.listdir('../sermons/preachingtoday'):\n",
    "        f=open('../sermons/preachingtoday/'+ sermon, 'rb')\n",
    "        contents =f.read()\n",
    "        contents = json.loads(contents)\n",
    "        tokens = clean_doc(contents['text'])\n",
    "        for word in range(len(tokens)):\n",
    "            if tokens[word] != '.':\n",
    "                data.append(tokens[word].lower())\n",
    "    return data\n",
    "\n",
    "def build_dataset(words, n_words):\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "def generate_batch(data, batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    context = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1  # [ skip_window input_word skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window  # input word at the center of the buffer\n",
    "        targets_to_avoid = [skip_window]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]  # this is the input word\n",
    "            context[i * num_skips + j, 0] = buffer[target]  # these are the context words\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, context\n",
    "\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = read_data()\n",
    "data, count, unused_dictionary, reverse_dictionary = build_dataset(\n",
    "vocabulary, vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "data_index = 0\n",
    "embedding_size = 128  # Dimension of the embedding vector.\n",
    "skip_window = 1  # How many words to consider left and right.\n",
    "num_skips = 2  # How many times to reuse an input to generate a label.\n",
    "num_sampled = 64  # Number of negative examples to sample.\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit\n",
    "# the validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent. These 3 variables are used only for\n",
    "# displaying model accuracy, they don't affect calculation.\n",
    "valid_size = 16  # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    with tf.name_scope('inputs'):\n",
    "        train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "        train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "        valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "    # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "    with tf.device('/cpu:0'):\n",
    "      # Look up embeddings for inputs.\n",
    "        with tf.name_scope('embeddings'):\n",
    "            embeddings = tf.Variable(\n",
    "                tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "            embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "      # Construct the variables for the NCE loss\n",
    "        with tf.name_scope('weights'):\n",
    "            nce_weights = tf.Variable(\n",
    "                tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                                stddev=1.0 / math.sqrt(embedding_size)))\n",
    "        with tf.name_scope('biases'):\n",
    "            nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Compute the average NCE loss for the batch.\n",
    "    # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "    # time we evaluate the loss.\n",
    "    # Explanation of the meaning of NCE loss:\n",
    "    #   http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.nce_loss(\n",
    "              weights=nce_weights,\n",
    "              biases=nce_biases,\n",
    "              labels=train_labels,\n",
    "              inputs=embed,\n",
    "              num_sampled=num_sampled,\n",
    "              num_classes=vocabulary_size))\n",
    "\n",
    "    # Add the loss value as a scalar to summary.\n",
    "    tf.summary.scalar('loss', loss)\n",
    "\n",
    "    # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "    with tf.name_scope('optimizer'):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "    # Compute the cosine similarity between minibatch examples and all\n",
    "    # embeddings.\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings,\n",
    "                                              valid_dataset)\n",
    "    similarity = tf.matmul(\n",
    "        valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "    # Merge all summaries.\n",
    "    merged = tf.summary.merge_all()\n",
    "\n",
    "    # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Create a saver.\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step  0 :  274.5562744140625\n",
      "Nearest to him: turmoil, sexualizing, inauguration, slanderous, infinity, tormented, anencouraging, netresourcessermonsdetailthecounterintuitivelifematt,\n",
      "Nearest to an: damnable, trial, disdainfully, receiver, 4:45, iraqi, 13:18, chatter,\n",
      "Nearest to he: tounderstand, site, caseywww, sexthing, operator, timeframe, neighborhelping, yeslord,\n",
      "Nearest to those: nickel, weakening, bonded, shamed, ourselveseven, youpsychologically, hypotheses, slandersaying,\n",
      "Nearest to christ: lexus, wellness, awake, vickilynn, garbled, pollyannaism, craigs, meteor,\n",
      "Nearest to our: adulteryis, nova, anguishing, thefailures, giventhat, williamson, condemnation, creaturea,\n",
      "Nearest to if: ifsomeone, seventy, mat, himthreshed, practicesaying, lesstwo, blueblooded, modest,\n",
      "Nearest to by: growingjoash, swarming, priestesses, eikomacrn, ofhim, theother, stuffwe, 1:38,\n",
      "Nearest to can: hereborn, marveling, georg, apartheid, thebest, casks, boundariedat, iliad,\n",
      "Nearest to good: cupdo, mercys, revive, sock, belz, thejust, disfigured, alwayssomething,\n",
      "Nearest to not: offmost, oppressionwhether, accurate, feet, limply, belongs, pastorally, sensibility,\n",
      "Nearest to would: attacked, hmmm, hohum, eightyyears, endurance, smashing, unfulfillment, civilian,\n",
      "Nearest to us: incorporates, situationwhen, symbolically, overlooks, 7:35asex, adventis, skilled, homily,\n",
      "Nearest to these: immunity, ways, superintendent, almostsick, basement, gilgal, godhas, managed,\n",
      "Nearest to love: hesaid, manages, shelfmake, calmly, 1666, kmart, behalf, thatmeans,\n",
      "Nearest to out: disdainfully, worker, manpoor, pilots, 19:2, 11:2, roots, ranger,\n",
      "Average loss at step  2000 :  111.34177600955964\n",
      "Average loss at step  4000 :  50.897507697820664\n",
      "Average loss at step  6000 :  32.96322311091423\n",
      "Average loss at step  8000 :  22.248424972295762\n",
      "Average loss at step  10000 :  17.484967344880104\n",
      "Nearest to him: us, you, turmoil, god, turning, skipping, property, redefined,\n",
      "Nearest to an: trial, stone, decided, the, writing, surely, restrain, mega,\n",
      "Nearest to he: i, she, they, it, jesus, god, tablet, we,\n",
      "Nearest to those: done, mega, daves, his, whats, prepared, torah, these,\n",
      "Nearest to christ: awake, hendricks, doubted, edge, head, imperishable, skylight, anxiety,\n",
      "Nearest to our: their, the, al, your, his, condemnation, games, foundational,\n",
      "Nearest to if: j, gehenna, remind, rocket, and, stops, veterans, ought,\n",
      "Nearest to by: contemplation, lineage, marshmallows, of, exodus, addressed, is, commissioned,\n",
      "Nearest to can: will, would, must, may, to, sweeping, and, rahab,\n",
      "Nearest to good: unless, godly, revive, talk, combine, loves, order, center,\n",
      "Nearest to not: dentist, divinity, depressed, to, crucified, rembrandt, tools, jeremiahs,\n",
      "Nearest to would: will, can, attacked, betrayal, to, could, toys, itch,\n",
      "Nearest to us: him, you, symbolically, danger, hamas, them, group, eighth,\n",
      "Nearest to these: fulfillment, ways, prepared, basement, individual, longing, those, racing,\n",
      "Nearest to love: behalf, building, natural, hesaid, cambodia, 50, die, pagans,\n",
      "Nearest to out: skipping, shaphan, back, mean, draw, d, believed, characteristics,\n",
      "Average loss at step  12000 :  14.065465301394463\n",
      "Average loss at step  14000 :  11.571025089621545\n",
      "Average loss at step  16000 :  10.380148221611977\n",
      "Average loss at step  18000 :  9.260261989831925\n",
      "Average loss at step  20000 :  8.129186166286468\n",
      "Nearest to him: us, me, you, them, god, turmoil, skipping, cabinet,\n",
      "Nearest to an: trial, a, tammy, the, demonstrated, decree, writing, cagney,\n",
      "Nearest to he: she, i, it, they, god, jesus, we, this,\n",
      "Nearest to those: these, you, his, mega, prepared, gong, attachment, pardons,\n",
      "Nearest to christ: awake, hendricks, doubted, edge, formless, 111, laurels, squad,\n",
      "Nearest to our: their, your, his, the, my, al, foundational, gods,\n",
      "Nearest to if: and, interceding, that, gehenna, when, because, but, stops,\n",
      "Nearest to by: is, for, lineage, without, contemplation, of, addressed, flourish,\n",
      "Nearest to can: will, would, must, may, to, could, recycled, cant,\n",
      "Nearest to good: godly, revive, unless, combine, butter, center, quintessential, at,\n",
      "Nearest to not: to, it, depressed, dentist, divinity, tools, hokmah, be,\n",
      "Nearest to would: will, can, could, betrayal, must, to, should, toys,\n",
      "Nearest to us: you, him, them, me, danger, symbolically, calgary, god,\n",
      "Nearest to these: those, fulfillment, ways, some, prepared, gilgal, you, the,\n",
      "Nearest to love: hesaid, natural, cambodia, behalf, building, storyline, die, 50,\n",
      "Nearest to out: back, skipping, characteristics, draw, mean, shaphan, 6:4, cat,\n",
      "Average loss at step  22000 :  8.809635322332381\n",
      "Average loss at step  24000 :  7.784263318300247\n",
      "Average loss at step  26000 :  7.01249380838871\n",
      "Average loss at step  28000 :  6.874211733698845\n",
      "Average loss at step  30000 :  6.9844376513957975\n",
      "Nearest to him: me, them, us, you, god, her, turmoil, skipping,\n",
      "Nearest to an: trial, demonstrated, zion, writing, decree, tammy, the, a,\n",
      "Nearest to he: she, i, it, they, jesus, god, we, paul,\n",
      "Nearest to those: these, his, the, you, birthright, gallant, mega, some,\n",
      "Nearest to christ: awake, hendricks, jesus, doubted, formless, god, edge, 111,\n",
      "Nearest to our: their, your, my, the, his, al, foundational, her,\n",
      "Nearest to if: when, but, because, and, gehenna, interceding, sure, where,\n",
      "Nearest to by: eucharistic, is, in, without, lineage, flourish, are, for,\n",
      "Nearest to can: will, would, could, may, must, to, cant, dont,\n",
      "Nearest to good: godly, insults, waders, combine, revive, hard, quintessential, unless,\n",
      "Nearest to not: to, you, it, be, really, dentist, depressed, perezuzza,\n",
      "Nearest to would: will, can, could, to, must, should, may, dont,\n",
      "Nearest to us: him, them, you, me, god, situationwhen, calgary, danger,\n",
      "Nearest to these: those, some, the, two, fulfillment, ways, prepared, his,\n",
      "Nearest to love: behalf, natural, cambodia, sprinkled, storyline, hesaid, render, building,\n",
      "Nearest to out: back, skipping, characteristics, draw, mean, pummeled, kindness, 6:4,\n",
      "Average loss at step  32000 :  5.886782319545746\n",
      "Average loss at step  34000 :  5.5770365098714825\n",
      "Average loss at step  36000 :  5.368265749692917\n",
      "Average loss at step  38000 :  5.169086616516113\n",
      "Average loss at step  40000 :  5.169517565608024\n",
      "Nearest to him: me, them, us, you, god, her, jesus, skipping,\n",
      "Nearest to an: demonstrated, zion, the, writing, decree, trial, tammy, 16:7,\n",
      "Nearest to he: she, i, they, it, jesus, paul, god, we,\n",
      "Nearest to those: these, some, the, his, you, produced, mega, birthright,\n",
      "Nearest to christ: awake, jesus, hendricks, doubted, lord, god, formless, 111,\n",
      "Nearest to our: their, your, my, his, the, foundational, her, al,\n",
      "Nearest to if: when, but, because, gehenna, maybe, vampire, now, sure,\n",
      "Nearest to by: without, is, in, eucharistic, with, for, trapeze, has,\n",
      "Nearest to can: will, would, could, must, may, cant, dont, to,\n",
      "Nearest to good: godly, revive, insults, hard, combine, quintessential, waders, mega,\n",
      "Nearest to not: to, you, be, it, really, firebomb, probably, dentist,\n",
      "Nearest to would: will, can, could, must, should, dont, may, to,\n",
      "Nearest to us: them, him, me, you, god, situationwhen, calgary, danger,\n",
      "Nearest to these: those, some, two, the, fulfillment, ways, his, prepared,\n",
      "Nearest to love: behalf, natural, sprinkled, cambodia, possessed, hesaid, storyline, childrens,\n",
      "Nearest to out: back, characteristics, skipping, into, draw, kindness, mean, down,\n",
      "Average loss at step  42000 :  5.168684623599052\n",
      "Average loss at step  44000 :  5.13475010740757\n",
      "Average loss at step  46000 :  5.1396361275911335\n",
      "Average loss at step  48000 :  5.0719988677501675\n",
      "Average loss at step  50000 :  4.910964158058166\n",
      "Nearest to him: me, them, us, her, you, god, it, jesus,\n",
      "Nearest to an: the, demonstrated, zion, writing, decree, trial, 16:7, pulpits,\n",
      "Nearest to he: she, i, they, it, jesus, god, we, paul,\n",
      "Nearest to those: these, some, people, the, produced, their, are, you,\n",
      "Nearest to christ: jesus, awake, lord, god, formless, hendricks, doubted, 111,\n",
      "Nearest to our: their, your, my, his, the, foundational, her, this,\n",
      "Nearest to if: when, but, maybe, because, then, where, gehenna, vampire,\n",
      "Nearest to by: without, in, is, eucharistic, with, trapeze, has, be,\n",
      "Nearest to can: will, would, could, may, must, cant, dont, might,\n",
      "Nearest to good: godly, hard, combine, insults, revive, quintessential, waders, hole,\n",
      "Nearest to not: to, you, it, firebomb, probably, really, dentist, depressed,\n",
      "Nearest to would: will, can, could, should, must, may, dont, might,\n",
      "Nearest to us: them, me, him, you, god, danger, it, situationwhen,\n",
      "Nearest to these: those, some, two, the, you, ways, fulfillment, three,\n",
      "Nearest to love: behalf, sprinkled, natural, cambodia, possessed, widget, die, irony,\n",
      "Nearest to out: back, golda, down, characteristics, teflon, skipping, into, kindness,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  52000 :  4.998522626280785\n",
      "Average loss at step  54000 :  4.859140347123146\n",
      "Average loss at step  56000 :  4.862546933293342\n",
      "Average loss at step  58000 :  4.76999607694149\n",
      "Average loss at step  60000 :  4.812239512205124\n",
      "Nearest to him: me, them, us, you, her, god, inches, people,\n",
      "Nearest to an: demonstrated, zion, writing, 16:7, pulpits, decree, 5david, trial,\n",
      "Nearest to he: she, i, they, it, jesus, god, we, paul,\n",
      "Nearest to those: these, some, you, people, produced, the, their, birthright,\n",
      "Nearest to christ: jesus, lord, awake, god, hendricks, formless, laurels, 111,\n",
      "Nearest to our: their, your, my, the, his, foundational, this, her,\n",
      "Nearest to if: when, maybe, but, because, then, now, where, vampire,\n",
      "Nearest to by: without, in, eucharistic, with, is, has, was, trapeze,\n",
      "Nearest to can: will, could, would, may, must, cant, might, dont,\n",
      "Nearest to good: hard, godly, combine, quentin, revive, insults, delay, quintessential,\n",
      "Nearest to not: it, be, you, probably, really, dentist, depressed, to,\n",
      "Nearest to would: will, can, could, should, must, may, might, dont,\n",
      "Nearest to us: them, me, him, you, god, calgary, danger, skipping,\n",
      "Nearest to these: those, some, two, the, three, you, ways, mega,\n",
      "Nearest to love: cambodia, behalf, widget, sprinkled, natural, possessed, penn, see,\n",
      "Nearest to out: back, down, teflon, golda, into, up, characteristics, skipping,\n",
      "Average loss at step  62000 :  4.798755007982254\n",
      "Average loss at step  64000 :  4.9298909202814105\n",
      "Average loss at step  66000 :  4.88906738448143\n",
      "Average loss at step  68000 :  4.887494503259659\n",
      "Average loss at step  70000 :  4.622411189198494\n",
      "Nearest to him: me, them, us, her, god, you, inches, himself,\n",
      "Nearest to an: demonstrated, 5david, zion, writing, decree, 16:7, completely, fringe,\n",
      "Nearest to he: she, i, they, jesus, it, paul, we, god,\n",
      "Nearest to those: these, some, you, the, two, people, produced, mega,\n",
      "Nearest to christ: lord, god, awake, jesus, him, recommend, formless, squad,\n",
      "Nearest to our: their, your, my, his, the, foundational, her, sevenday,\n",
      "Nearest to if: when, maybe, now, then, because, where, do, but,\n",
      "Nearest to by: without, eucharistic, in, with, trapeze, is, has, flourish,\n",
      "Nearest to can: will, could, would, may, must, cant, might, should,\n",
      "Nearest to good: hard, godly, quentin, combine, insults, revive, quintessential, erected,\n",
      "Nearest to not: probably, it, you, be, nerd, depressed, really, to,\n",
      "Nearest to would: will, can, could, should, may, must, might, dont,\n",
      "Nearest to us: them, me, him, you, god, calgary, others, levels,\n",
      "Nearest to these: those, some, two, the, three, mega, gilgal, four,\n",
      "Nearest to love: cambodia, behalf, natural, possessed, widget, sprinkled, see, storyline,\n",
      "Nearest to out: back, down, golda, teflon, into, up, off, skipping,\n",
      "Average loss at step  72000 :  4.714489868164063\n",
      "Average loss at step  74000 :  4.675775906085968\n",
      "Average loss at step  76000 :  4.652969963908196\n",
      "Average loss at step  78000 :  4.741332967877388\n",
      "Average loss at step  80000 :  4.723787794351578\n",
      "Nearest to him: me, them, us, her, you, god, himself, inches,\n",
      "Nearest to an: demonstrated, 5david, zion, decree, 16:7, fringe, writing, pulpits,\n",
      "Nearest to he: she, they, i, jesus, paul, it, god, we,\n",
      "Nearest to those: these, some, people, the, two, produced, four, you,\n",
      "Nearest to christ: jesus, lord, god, awake, him, recommend, squad, shares,\n",
      "Nearest to our: their, your, my, his, the, foundational, sevenday, her,\n",
      "Nearest to if: when, maybe, because, now, then, where, but, vampire,\n",
      "Nearest to by: without, eucharistic, in, throughout, indentured, trapeze, swiped, youre,\n",
      "Nearest to can: will, could, would, must, may, cant, should, might,\n",
      "Nearest to good: hard, quentin, combine, insults, godly, revive, great, erected,\n",
      "Nearest to not: it, you, probably, to, also, god, firebomb, always,\n",
      "Nearest to would: will, can, could, should, might, may, must, didnt,\n",
      "Nearest to us: them, me, him, you, god, situationwhen, calgary, levels,\n",
      "Nearest to these: those, some, two, three, the, mega, four, ways,\n",
      "Nearest to love: cambodia, storyline, behalf, natural, sprinkled, help, investments, widget,\n",
      "Nearest to out: back, down, golda, teflon, up, into, off, skipping,\n",
      "Average loss at step  82000 :  4.668523547053337\n",
      "Average loss at step  84000 :  4.813432384967804\n",
      "Average loss at step  86000 :  4.856520273208618\n",
      "Average loss at step  88000 :  4.621128119945526\n",
      "Average loss at step  90000 :  4.6616837084293365\n",
      "Nearest to him: me, them, us, her, god, you, himself, jesus,\n",
      "Nearest to an: demonstrated, zion, 5david, 16:7, pulpits, decree, writing, fringe,\n",
      "Nearest to he: she, i, it, they, paul, jesus, we, god,\n",
      "Nearest to those: these, some, people, produced, two, four, you, many,\n",
      "Nearest to christ: god, lord, jesus, awake, him, recommend, formless, squad,\n",
      "Nearest to our: their, your, my, his, the, her, foundational, sevenday,\n",
      "Nearest to if: when, maybe, now, because, then, where, but, vampire,\n",
      "Nearest to by: without, eucharistic, trapeze, indentured, swiped, flourish, throughout, has,\n",
      "Nearest to can: will, could, would, must, may, cant, should, might,\n",
      "Nearest to good: hard, quentin, godly, great, combine, insults, stupid, quintessential,\n",
      "Nearest to not: probably, also, be, solidified, depressed, proactively, you, dentist,\n",
      "Nearest to would: will, can, could, should, may, might, must, didnt,\n",
      "Nearest to us: them, me, him, you, god, situationwhen, calgary, others,\n",
      "Nearest to these: those, some, two, four, three, many, mega, the,\n",
      "Nearest to love: cambodia, see, help, storyline, behalf, widget, sprinkled, investments,\n",
      "Nearest to out: back, down, into, off, golda, teflon, skipping, characteristics,\n",
      "Average loss at step  92000 :  4.701574896097183\n",
      "Average loss at step  94000 :  4.596443375825882\n",
      "Average loss at step  96000 :  4.426130561470986\n",
      "Average loss at step  98000 :  4.393190175652504\n",
      "Average loss at step  100000 :  4.398957509756088\n",
      "Nearest to him: them, me, us, her, you, god, others, jesus,\n",
      "Nearest to an: demonstrated, 5david, zion, pulpits, 16:7, decree, writing, 50yearold,\n",
      "Nearest to he: she, they, paul, i, it, jesus, we, david,\n",
      "Nearest to those: these, some, people, the, produced, two, many, mega,\n",
      "Nearest to christ: lord, jesus, god, awake, recommend, him, served, squad,\n",
      "Nearest to our: their, your, my, the, his, her, foundational, sevenday,\n",
      "Nearest to if: when, maybe, then, now, because, vampire, but, where,\n",
      "Nearest to by: without, eucharistic, is, trapeze, with, has, indentured, swiped,\n",
      "Nearest to can: will, could, would, must, may, cant, should, might,\n",
      "Nearest to good: hard, godly, quentin, best, stupid, great, insults, combine,\n",
      "Nearest to not: you, never, probably, also, nerd, to, be, really,\n",
      "Nearest to would: will, can, could, should, may, must, might, dont,\n",
      "Nearest to us: them, me, him, you, god, situationwhen, others, her,\n",
      "Nearest to these: those, some, two, the, many, three, four, mega,\n",
      "Nearest to love: cambodia, widget, help, see, behalf, certainty, investments, righteousness,\n",
      "Nearest to out: back, down, off, into, teflon, golda, up, doorway,\n"
     ]
    }
   ],
   "source": [
    "num_steps = 100001\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # We must initialize all variables before we use them.\n",
    "    init.run()\n",
    "    print('Initialized')\n",
    "    skip_window = 1\n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batch_inputs, batch_labels = generate_batch(data, batch_size, num_skips,skip_window)\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "      # Define metadata variable.\n",
    "        run_metadata = tf.RunMetadata()\n",
    "\n",
    "      # We perform one update step by evaluating the optimizer op (including it\n",
    "      # in the list of returned values for session.run()\n",
    "      # Also, evaluate the merged op to get all summaries from the returned\n",
    "      # \"summary\" variable. Feed metadata variable to session for visualizing\n",
    "      # the graph in TensorBoard.\n",
    "        _, summary, loss_val = session.run([optimizer, merged, loss],\n",
    "                                         feed_dict=feed_dict,\n",
    "                                         run_metadata=run_metadata)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "        # The average loss is an estimate of the loss over the last 2000\n",
    "        # batches.\n",
    "            print('Average loss at step ', step, ': ', average_loss)\n",
    "            average_loss = 0\n",
    "        if step % 10000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            for i in range(valid_size):\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                top_k = 8  # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                log_str = 'Nearest to %s:' % valid_word\n",
    "                for k in range(top_k):\n",
    "                    close_word = reverse_dictionary[nearest[k]]\n",
    "                    log_str = '%s %s,' % (log_str, close_word)\n",
    "                print(log_str)\n",
    "    if step == num_steps-1:\n",
    "        final_embeddings = normalized_embeddings.eval(session = session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dictionary = {}\n",
    "for word in range(len(reverse_dictionary)):\n",
    "    final_dictionary[reverse_dictionary[word]] = final_embeddings[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('embeddings.pkl', 'wb') as handle:\n",
    "    pickle.dump(final_dictionary, handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
