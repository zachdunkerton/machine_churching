{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(doc):\n",
    "    stemmed_doc = ''\n",
    "    for w in doc.split(' '):\n",
    "        stemmed_doc += stemmer.stem(w) + ' '\n",
    "    return stemmed_doc\n",
    "\n",
    "def train(data, iterations):\n",
    "    models = []\n",
    "    corpus = data['corpus']\n",
    "    labels = data['labels']\n",
    "    for i in range(iterations):\n",
    "        random_indicies = random.sample(data['c_w_corpus'], 500) +random.sample(data['piper_corpus'],500) +random.sample(data['spurgeon_corpus'], 500) +random.sample(data['tozer_corpus'], 500)\n",
    "        X = corpus[random_indicies,:]\n",
    "        y = [labels[i] for i in random_indicies]\n",
    "        print('Generating', i+1)\n",
    "        if i % 2 ==0:\n",
    "            result = generate_SVC_clasifier_wo_vect(X,y)\n",
    "        else:\n",
    "            result = generate_tree_clasifier(X,y)\n",
    "        models.append(result)\n",
    "    with open('models.pkl', 'wb') as f:\n",
    "        pickle.dump(models, f)\n",
    "        \n",
    "def predict(X, y, vect):\n",
    "    models =pickle.load( open( \"models.pkl\", \"rb\" ) )\n",
    "    preds = []\n",
    "    final_prediction = []\n",
    "    print('Vectorizing Input')\n",
    "    X = vect.transform(X)\n",
    "    print('Predicting')\n",
    "    for m in models: \n",
    "        preds.append(m.predict(X))\n",
    "    for i in range(len(preds[0])):\n",
    "        this_pred = []\n",
    "        for j in range(len(preds)):\n",
    "            this_pred.append(preds[j][i])\n",
    "        final_prediction.append(most_common(this_pred))\n",
    "    print(\"Multinomial NB: \" , round(accuracy_score(y, final_prediction), 3))\n",
    "    print(classification_report(y, final_prediction, labels=range(0,4)))\n",
    "    \n",
    "def generate_SVC_clasifier_wo_vect(X, y):\n",
    "    print('Fitting SVC')\n",
    "    svc = SVC()#, class_weight = weight_dict)\n",
    "    svc.fit(X, y)\n",
    "    return svc\n",
    "\n",
    "def generate_tree_clasifier(X, y):\n",
    "    print('Fitting Tree')\n",
    "    tree = DecisionTreeClassifier(max_depth = 10)\n",
    "    tree.fit(X, y)\n",
    "    return tree\n",
    "\n",
    "def most_common(lst):\n",
    "    return max(set(lst), key=lst.count)\n",
    "\n",
    "def load_sermons():\n",
    "    data = []\n",
    "    labels = []\n",
    "    label= 0\n",
    "    count=0\n",
    "    authors = ['charles_wesley', 'spurgeon', 'tozer', 'piper']\n",
    "    for author in authors:\n",
    "        for sermon in os.listdir('./sermons/' + author):   \n",
    "            #os.remove('./sermons/' + author + '/.DS_Store')\n",
    "            f=open('./sermons/'+ author + '/' + sermon, \"r\")\n",
    "            if f.mode == 'r':\n",
    "                contents =f.read()\n",
    "                if len(contents) > 100:\n",
    "                    contents = stem_words(contents)\n",
    "                    data.append(contents)\n",
    "                    labels.append(label)\n",
    "            count +=1\n",
    "        print(label, author)\n",
    "        label +=1\n",
    "        count = 0\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.23)\n",
    "    c_w = [] \n",
    "    piper = [] \n",
    "    spurgeon=[] \n",
    "    tozer = [] \n",
    "\n",
    "    for i in range(len(y_train)):\n",
    "        if y_train[i] == 0:\n",
    "            c_w.append((X_train[i], 0))\n",
    "        elif y_train[i] == 1:\n",
    "            spurgeon.append((X_train[i], 1))\n",
    "        elif y_train[i] == 2:\n",
    "            tozer.append((X_train[i], 2))\n",
    "        elif y_train[i] == 3:\n",
    "            piper.append((X_train[i], 3))\n",
    "            \n",
    "    X = random.sample(c_w, 500) +random.sample(piper,500) +random.sample(spurgeon, 500) +random.sample(tozer, 500)+ random.sample(c_w, 500) +random.sample(piper,500) +random.sample(spurgeon, 500) +random.sample(tozer, 500)\n",
    "    np.random.shuffle(X)\n",
    "    X, y = [item[0] for item in X], [item[1] for item in X]\n",
    "    vect = TfidfVectorizer(stop_words = \"english\", ngram_range = {1,4})\n",
    "    corpus = vect.fit_transform(X)\n",
    "\n",
    "    c_w_corpus = [] \n",
    "    piper_corpus = [] \n",
    "    spurgeon_corpus =[] \n",
    "    tozer_corpus = [] \n",
    "\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == 0:\n",
    "            c_w_corpus.append(i)\n",
    "        elif y[i] == 1:\n",
    "            spurgeon_corpus.append(i)\n",
    "        elif y[i] == 2:\n",
    "            tozer_corpus.append(i)\n",
    "        elif y[i] == 3:\n",
    "            piper_corpus.append(i)\n",
    "    return {'c_w_corpus':c_w_corpus, 'piper_corpus':piper_corpus, \n",
    "            'spurgeon_corpus':spurgeon_corpus, 'tozer_corpus':tozer_corpus, 'corpus':corpus, \n",
    "            'X_test':X_test, 'y_test':y_test, 'labels':y, 'vect': vect}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 charles_wesley\n",
      "1 spurgeon\n",
      "2 tozer\n",
      "3 piper\n",
      "Generating 1\n",
      "Fitting SVC\n",
      "Generating 2\n",
      "Fitting Tree\n",
      "Generating 3\n",
      "Fitting SVC\n",
      "Generating 4\n",
      "Fitting Tree\n",
      "Generating 5\n",
      "Fitting SVC\n",
      "Generating 6\n",
      "Fitting Tree\n",
      "Generating 7\n",
      "Fitting SVC\n",
      "Generating 8\n",
      "Fitting Tree\n",
      "Generating 9\n",
      "Fitting SVC\n",
      "Generating 10\n",
      "Fitting Tree\n",
      "Vectorizing Input\n",
      "Predicting\n",
      "Multinomial NB:  0.873\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.99      0.89       177\n",
      "          1       0.92      0.61      0.74       153\n",
      "          2       0.74      0.95      0.83       207\n",
      "          3       0.98      0.88      0.93       455\n",
      "\n",
      "avg / total       0.89      0.87      0.87       992\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = load_sermons()\n",
    "models = train(data, 10)\n",
    "predict(data['X_test'], data['y_test'], data['vect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f=open('./sermons/john_wesley/john_wesley_Duty_of_Constant_Communion.txt', \"r\")\n",
    "if f.mode == 'r':\n",
    "    contents = [f.read() ]\n",
    "\n",
    "kyle = vect.transform(contents)\n",
    "prob = nb.predict(kyle)\n",
    "prob\n",
    "#for i in range(0,4):\n",
    "    #print(authors[i], \" : \" , prob[0][i]*100 , \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
